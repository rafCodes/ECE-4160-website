<!doctype html>
<html>
<!doctype html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <title>Raphael Fortuna ECE 4160 Website</title>
  <link rel="stylesheet" href="stylesheets/styles.css">
  <link rel="stylesheet" href="stylesheets/github-dark.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
  <script src="javascripts/respond.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

</head>

<body>
  <div id="header">
    <nav>
      <li class="fork"><a href="https://github.com/rafCodes/ECE-4160-website">View On GitHub</a></li>
    </nav>
  </div><!-- end header -->

  <div class="wrapper">

    <section>
      <div id="title">
        <h1>ECE 4160 Fast Robots</h1>
        <p></p>
        <center>
          <h2>Raphael Fortuna</h2>
        </center>
        <p></p>
        <hr>
        <span class="credits left">Lab reports made by <a href="https://github.com/rafCodes">Raphael Fortuna @
            rafCodes</a></span>
        <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a
            href="https://twitter.com/michigangraham">mattgraham</a></span>
      </div>

      <!-- Lab information goes here -->

      <center>
        <h2><a href="index.html" class="btn">Home</a></h2>
        <h1> Lab 10: Grid Localization using Bayes Filter </h1>
      </center>

      <div>
        <p>
          <h2> Summary </h2>
          <h3>
            In this lab, I implemented grid localization using a Bayes Filter in the provided simulation. This uses probabilistic methods for localization as non-probabilistic can be unreliable and probabilistic methods seek to improve the accuracy of the robotics location.
          </h3>
        </p>
      </div>

      <div>
        <p>
          <h2> Prelab </h2>
          <h3>
            The prelab involved going through the provided background information and reviewing the Bayes Filter algorithm. The background information included the terminology for robot localization, grid localization, gaussian model for sensor measurement noise, and motion model for relative movement changes between time steps.
          </h3>
          

          <h3>
            The grid localization is partitioned into a discretized x, y, and theta space – 12, 8, and 18 respectively –  to allow for a short computation time and each x, y, theta triple corresponds to a robot location and orientation in the space and the probability of it being there, with the sum over all triples being 1.
          </h3>

          <h3>
            The Bayes filter algorithm is a cycle of prediction of where the robot thinks it is based on its movement (prior belief) followed by an update step to reduce the uncertainty of where the robot thinks it is. The update step uses the data collected by the sensors to do this.
          </h3>

          <h3>
            The image below shows the equation for the Bayes filter algorithm from lecture 21.
          </h3>

          <div class = img_parent>
            <img src="Labs\Lab 10\BayesFilterAlgorithm.png" alt="Bayes Filter Algorithm">
          <p></p>
          </div>

        </p>
      </div>

      <div>
        <p>
          <h2> Codebase </h2>
          <h3>
            The code being used was from the provided simulation in class with two additions – importing numpy and math for the Bayes filter calculations. The file used for the lab was provided and had five classes: a commander class to interface with the simulator and plotting, a VirtualRobot class to interface with the robot and its sensors, a mapper class that bundles information and functions for the grid map, BaseLocalization bundling localization functions and ToF data collection, and the Trajectory class that has a path that it manages and has the robot execute. I did not have to modify the Trajectory as my robot did not crash while executing the trajectory.
          </h3>
        </p>
      </div>

      <div>
        <p>
          <h2> Localization Task: Compute Control </h2>
          
          <h3>
            The first function was compute_control that took in the current and previous pose of the robot and got the control information using the odometry motion model. Each pose is composed of a x, y, and yaw the final output should be a rotation, translation, and rotation that described the change in position and direction between the two poses. The image below from lecture 17 describes how the rotations and translations interact with the two poses. 
          </h3>

          <h3>
            The first rotation is to move from the initial direction to the direction of the other pose. 
          </h3>

          <h3>
            The translation is the movement between the first and second pose. 
          </h3>

          <h3>
            And the second rotation is to align the robot to the correct direction at the second pose. I found the difference between the current angle and the angle – as well as distance - needed to translate from pose 1 to pose 2 and then got the last rotation from what the final orientation should be.
          </h3>

          <div class = img_parent>
            <img src="Labs\Lab 10\ComputeControlDiagram.png" alt="Diagram relating roation 1, translation 1, and rotation 2">
          <p></p>
          </div>

          <script src="https://gist.github.com/rafCodes/0c9c888101a99ce234102748c86c9ccd.js"></script>

        </p>
      </div>

      <div>
        <p>
          <h2> Localization Task: Odometry Motion Model </h2>
          <h3>
            The odom_motion_model function was used in finding the prior beliefs of the Bayes Filter based on the movement of the robot from one pose to another through prediction. We used the necessary control action, u, and the actual_u computed from the two poses to determine how likely the transition was from the previous pose to the current pose though multiple gaussian functions multiplied together and described below from the lab guide:
          </h3>

          <div class = img_parent>
            <img src="Labs\Lab 10\GaussianFormulaOdom.png" alt="Gaussian Formula for Odometry">
          <p></p>
          </div>

          <h3>
            Translating this into code is shown below:
          </h3>
          <script src="https://gist.github.com/rafCodes/4d8b343b367a70fe7db83c363f0ec647.js"></script>

        </p>
      </div>

      <div>
        <p>
          <h2> Localization Task: Updating Prior Belief </h2>
          <h3>
            This function was used to update all the prior beliefs, bel bar, given a previous location and looked the probability of traveling to every location from the given previous location. It used the odom_motion_model function for getting the transition probability. The photo below shows what section of the Bayes Filter is being computed from lecture 21.
          </h3>

          <div class = img_parent>
            <img src="Labs\Lab 10\BelBarEquation.png" alt="Prediction step of Bayes Filter">
          <p></p>
          </div>

          
          <h3>
            The code for this is shown below:
          </h3>
          <script src="https://gist.github.com/rafCodes/9fd469252cc3705b98a55e3664d7ec52.js"></script>

        </p>
      </div>

      <div>
        <p>
          <h2> Localization Task: Prediction Step </h2>
          <h3>
            The next function iterated over all the poses with their previous beliefs and updated the prior belief using the previous function. This is the prediction step of the Bayes Filter. When iterating though the different locations, the beliefs that are under .0001 are skipped since those are too small to have an impact and will be near 0 when updating the prior belief with them. At the end, the previous beliefs are normalized since it is a probability distribution and must sum to 0.
          </h3>

          <script src="https://gist.github.com/rafCodes/a600217324ddb2e937c703f82a3a9b7e.js"></script>

        </p>
      </div>

      <div>
        <p>
          <h2> Localization Task: Sensor Model </h2>
          <h3>
            Moving on to the update step of the Bayes Filter, this next function is used to get the likelihood of getting each of the sensor’s measurements given what the actual measurements would be for a given certain location’s pre-cached sensor data. This is to determine what locations would the sensor measurements be the most likely to come from, p(z|t).
          </h3>

          <script src="https://gist.github.com/rafCodes/42517135f900f854a1db24f65620f908.js"></script>
          
        </p>
      </div>

      <div>
        <p>
          <h2> Localization Task: Update Step </h2>
          <h3>
            The update step is the second half of the Bayes Filter and updates the previous beliefs, with the equation it describes shown below from lecture 21. 
          </h3>

          <div class = img_parent>
            <img src="Labs\Lab 10\UpdateStepEquation.png" alt="Update Step for Bayes Filter">
          <p></p>
          </div>

          <h3>
            Each point was iterated over and updated using the sensor model to first get the probability p(z|x) and then multiplied with the prediction step belief probability for that location to get the new belief for that location. After all points had been updated, the belief was normalized so that the total probability of the belief was 1.
          </h3>

          <script src="https://gist.github.com/rafCodes/5720bc2fb5bd8534a2e311142a90dadd.js"></script>

        </p>
      </div>
      <div>
        <p>
          <h2> Running the Bayes Filter Task </h2>
          <h3>
            I ran the Bayes Filter over the pre-programmed trajectory and recorded the plotter, simulation, and Jupyter notebook output. In the plotter, the green line was the ground truth of the robot, blue was the predicted location from the bayes filter, and the odometry readings were in red. The second video had squares on the map that represented that probability the robot was in that location, with white having the highest probability of having the robot. Below I have a photo of the completed trajectory and the two videos of the robot driving around.
          </h3>

          <div class = img_parent>
            <img src="Labs\Lab 10\TrajectoryGraph.png" alt="Simulation plot of where the robot drove around">
          <p></p>
          </div>

          <h3>
            Video of the robot driving around, no probability squares:
          </h3>

          <video width="430" height="270" controls="">
            <source src="Labs\Lab 10\LocalizationNoGraph.mp4">
          <p></p>
          </video>

          <h3>
            Video with squares showing probabilities of where the robot is:
          </h3>

          <video width="430" height="270" controls="">
            <source src="Labs\Lab 10\LocalizationWithGraph.mp4">
          <p></p>
          </video>

          <h3>
            Below is the output of the Jupyter notebook after each time the Bayes Filter was run with the probability of the most probable state and the comparison between the ground truth pose. I found that the probabilities seemed really small for that index on the order of .0001, but it is in comparison to all the other 1943 locations that the robot could be in – as specified from the lab handout. The error for angle was very high and appeared to be summing the angle each time instead of normalizing it to +/- 180 before comparing it. The index of position seemed to line up well and were at most off by 1 in the x, y, and theta indices. This translated to at most 35 cm of error between the ground truth pose and the belief pose, with errors averaging around 5-15 cm. Looking at the photo of the movement in the above graph, whenever it did a turn that was not 90 degrees or when moving through a narrow area, struggled to localize itself and it encountered the most error with where the robot believed it was.          </h3>
          <h3>
            Only update text:
          </h3>

          <script src="https://gist.github.com/rafCodes/4e0490ae59f74b94d43dbf0446eb982e.js"></script>

          <h3>
            All output text:
          </h3>
          
          <script src="https://gist.github.com/rafCodes/0321b4e4b528b1c5a67e79730054460d.js"></script>


        </p>
      </div>

  </div>
  </section>

  </div>

</body>

</html>